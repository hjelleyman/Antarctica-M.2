{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying for data:\n",
      "\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=NSIDC-0051&version=001&version=01&version=1&temporal[]=1978-10-26T00:00:00Z,2019-12-31T23:59:59Z&bounding_box=-180,-90,180,-30\n",
      "\n",
      "Found 13885 matches.\n",
      ".......\n",
      "Downloading 494 files...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# ----------------------------------------------------------------------------\n",
    "# NSIDC Data Download Script\n",
    "#\n",
    "# Copyright (c) 2020 Regents of the University of Colorado\n",
    "# Permission is hereby granted, free of charge, to any person obtaining\n",
    "# a copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "# The above copyright notice and this permission notice shall be included\n",
    "# in all copies or substantial portions of the Software.\n",
    "#\n",
    "# Tested in Python 2.7 and Python 3.4, 3.6, 3.7\n",
    "#\n",
    "# To run the script at a Linux, macOS, or Cygwin command-line terminal:\n",
    "#   $ python nsidc-data-download.py\n",
    "#\n",
    "# On Windows, open Start menu -> Run and type cmd. Then type:\n",
    "#     python nsidc-data-download.py\n",
    "#\n",
    "# The script will first search Earthdata for all matching files.\n",
    "# You will then be prompted for your Earthdata username/password\n",
    "# and the script will download the matching files.\n",
    "#\n",
    "# If you wish, you may store your Earthdata username/password in a .netrc\n",
    "# file in your $HOME directory and the script will automatically attempt to\n",
    "# read this file. The .netrc file should have the following format:\n",
    "#    machine urs.earthdata.nasa.gov login myusername password mypassword\n",
    "# where 'myusername' and 'mypassword' are your Earthdata credentials.\n",
    "#\n",
    "from __future__ import print_function\n",
    "\n",
    "import base64\n",
    "import itertools\n",
    "import json\n",
    "import netrc\n",
    "import ssl\n",
    "import sys\n",
    "from getpass import getpass\n",
    "\n",
    "try:\n",
    "    from urllib.parse import urlparse\n",
    "    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n",
    "    from urllib.error import HTTPError, URLError\n",
    "except ImportError:\n",
    "    from urlparse import urlparse\n",
    "    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n",
    "\n",
    "short_name = 'NSIDC-0051'\n",
    "version = '1'\n",
    "time_start = '1978-10-26T00:00:00Z'\n",
    "time_end = '2019-12-31T23:59:59Z'\n",
    "bounding_box = '-180,-90,180,-30'\n",
    "polygon = ''\n",
    "filename_filter = ''\n",
    "url_list = []\n",
    "\n",
    "CMR_URL = 'https://cmr.earthdata.nasa.gov'\n",
    "URS_URL = 'https://urs.earthdata.nasa.gov'\n",
    "CMR_PAGE_SIZE = 2000\n",
    "CMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n",
    "                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n",
    "                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n",
    "\n",
    "\n",
    "def get_username():\n",
    "    username = ''\n",
    "\n",
    "    # For Python 2/3 compatibility:\n",
    "    try:\n",
    "        do_input = raw_input  # noqa\n",
    "    except NameError:\n",
    "        do_input = input\n",
    "\n",
    "    while not username:\n",
    "        try:\n",
    "            username = do_input('Earthdata username: ')\n",
    "        except KeyboardInterrupt:\n",
    "            quit()\n",
    "    return username\n",
    "\n",
    "\n",
    "def get_password():\n",
    "    password = ''\n",
    "    while not password:\n",
    "        try:\n",
    "            password = getpass('password: ')\n",
    "        except KeyboardInterrupt:\n",
    "            quit()\n",
    "    return password\n",
    "\n",
    "\n",
    "def get_credentials(url):\n",
    "    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n",
    "    credentials = None\n",
    "    errprefix = ''\n",
    "    try:\n",
    "        info = netrc.netrc()\n",
    "        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n",
    "        errprefix = 'netrc error: '\n",
    "    except Exception as e:\n",
    "        if (not ('No such file' in str(e))):\n",
    "            print('netrc error: {0}'.format(str(e)))\n",
    "        username = None\n",
    "        password = None\n",
    "\n",
    "    while not credentials:\n",
    "        if not username:\n",
    "            username = get_username()\n",
    "            password = get_password()\n",
    "        credentials = '{0}:{1}'.format(username, password)\n",
    "        credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n",
    "\n",
    "        if url:\n",
    "            try:\n",
    "                req = Request(url)\n",
    "                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n",
    "                opener = build_opener(HTTPCookieProcessor())\n",
    "                opener.open(req)\n",
    "            except HTTPError:\n",
    "                print(errprefix + 'Incorrect username or password')\n",
    "                errprefix = ''\n",
    "                credentials = None\n",
    "                username = None\n",
    "                password = None\n",
    "\n",
    "    return credentials\n",
    "\n",
    "\n",
    "def build_version_query_params(version):\n",
    "    desired_pad_length = 3\n",
    "    if len(version) > desired_pad_length:\n",
    "        print('Version string too long: \"{0}\"'.format(version))\n",
    "        quit()\n",
    "\n",
    "    version = str(int(version))  # Strip off any leading zeros\n",
    "    query_params = ''\n",
    "\n",
    "    while len(version) <= desired_pad_length:\n",
    "        padded_version = version.zfill(desired_pad_length)\n",
    "        query_params += '&version={0}'.format(padded_version)\n",
    "        desired_pad_length -= 1\n",
    "    return query_params\n",
    "\n",
    "\n",
    "def build_cmr_query_url(short_name, version, time_start, time_end,\n",
    "                        bounding_box=None, polygon=None,\n",
    "                        filename_filter=None):\n",
    "    params = '&short_name={0}'.format(short_name)\n",
    "    params += build_version_query_params(version)\n",
    "    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n",
    "    if polygon:\n",
    "        params += '&polygon={0}'.format(polygon)\n",
    "    elif bounding_box:\n",
    "        params += '&bounding_box={0}'.format(bounding_box)\n",
    "    if filename_filter:\n",
    "        option = '&options[producer_granule_id][pattern]=true'\n",
    "        params += '&producer_granule_id[]={0}{1}'.format(filename_filter, option)\n",
    "    return CMR_FILE_URL + params\n",
    "\n",
    "\n",
    "def cmr_download(urls):\n",
    "    \"\"\"Download files from list of urls.\"\"\"\n",
    "    if not urls:\n",
    "        return\n",
    "\n",
    "    url_count = len(urls)\n",
    "    print('Downloading {0} files...'.format(url_count))\n",
    "    credentials = None\n",
    "\n",
    "    for index, url in enumerate(urls, start=1):\n",
    "        if not credentials and urlparse(url).scheme == 'https':\n",
    "            credentials = get_credentials(url)\n",
    "\n",
    "        filename = url.split('/')[-1]\n",
    "        print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n",
    "                                    url_count,\n",
    "                                    filename))\n",
    "\n",
    "        try:\n",
    "            # In Python 3 we could eliminate the opener and just do 2 lines:\n",
    "            # resp = requests.get(url, auth=(username, password))\n",
    "            # open(filename, 'wb').write(resp.content)\n",
    "            req = Request(url)\n",
    "            if credentials:\n",
    "                req.add_header('Authorization', 'Basic {0}'.format(credentials))\n",
    "            opener = build_opener(HTTPCookieProcessor())\n",
    "            data = opener.open(req).read()\n",
    "            open(filename, 'wb').write(data)\n",
    "        except HTTPError as e:\n",
    "            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n",
    "        except URLError as e:\n",
    "            print('URL error: {0}'.format(e.reason))\n",
    "        except IOError:\n",
    "            raise\n",
    "        except KeyboardInterrupt:\n",
    "            quit()\n",
    "\n",
    "\n",
    "def cmr_filter_urls(search_results):\n",
    "    \"\"\"Select only the desired data files from CMR response.\"\"\"\n",
    "    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n",
    "        return []\n",
    "\n",
    "    entries = [e['links']\n",
    "               for e in search_results['feed']['entry']\n",
    "               if 'links' in e]\n",
    "    # Flatten \"entries\" to a simple list of links\n",
    "    links = list(itertools.chain(*entries))\n",
    "\n",
    "    urls = []\n",
    "    unique_filenames = set()\n",
    "    for link in links:\n",
    "        if 'href' not in link:\n",
    "            # Exclude links with nothing to download\n",
    "            continue\n",
    "        if 'inherited' in link and link['inherited'] is True:\n",
    "            # Why are we excluding these links?\n",
    "            continue\n",
    "        if 'rel' in link and 'data#' not in link['rel']:\n",
    "            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n",
    "            continue\n",
    "\n",
    "        if 'title' in link and 'opendap' in link['title'].lower():\n",
    "            # Exclude OPeNDAP links--they are responsible for many duplicates\n",
    "            # This is a hack; when the metadata is updated to properly identify\n",
    "            # non-datapool links, we should be able to do this in a non-hack way\n",
    "            continue\n",
    "\n",
    "        filename = link['href'].split('/')[-1]\n",
    "        if filename in unique_filenames:\n",
    "            # Exclude links with duplicate filenames (they would overwrite)\n",
    "            continue\n",
    "        unique_filenames.add(filename)\n",
    "\n",
    "        urls.append(link['href'])\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "def cmr_search(short_name, version, time_start, time_end,\n",
    "               bounding_box='', polygon='', filename_filter=''):\n",
    "    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n",
    "    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n",
    "                                        time_start=time_start, time_end=time_end,\n",
    "                                        bounding_box=bounding_box,\n",
    "                                        polygon=polygon, filename_filter=filename_filter)\n",
    "    print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n",
    "\n",
    "    cmr_scroll_id = None\n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "    try:\n",
    "        urls = []\n",
    "        while True:\n",
    "            req = Request(cmr_query_url)\n",
    "            if cmr_scroll_id:\n",
    "                req.add_header('cmr-scroll-id', cmr_scroll_id)\n",
    "            response = urlopen(req, context=ctx)\n",
    "            if not cmr_scroll_id:\n",
    "                # Python 2 and 3 have different case for the http headers\n",
    "                headers = {k.lower(): v for k, v in dict(response.info()).items()}\n",
    "                cmr_scroll_id = headers['cmr-scroll-id']\n",
    "                hits = int(headers['cmr-hits'])\n",
    "                if hits > 0:\n",
    "                    print('Found {0} matches.'.format(hits))\n",
    "                else:\n",
    "                    print('Found no matches.')\n",
    "            search_page = response.read()\n",
    "            search_page = json.loads(search_page.decode('utf-8'))\n",
    "            url_scroll_results = cmr_filter_urls(search_page)\n",
    "            if not url_scroll_results:\n",
    "                break\n",
    "            if hits > CMR_PAGE_SIZE:\n",
    "                print('.', end='')\n",
    "                sys.stdout.flush()\n",
    "            urls += url_scroll_results\n",
    "\n",
    "        if hits > CMR_PAGE_SIZE:\n",
    "            print()\n",
    "        return urls\n",
    "    except KeyboardInterrupt:\n",
    "        quit()\n",
    "\n",
    "\n",
    "def main():\n",
    "    global short_name, version, time_start, time_end, bounding_box, \\\n",
    "        polygon, filename_filter, url_list\n",
    "\n",
    "    # Supply some default search parameters, just for testing purposes.\n",
    "    # These are only used if the parameters aren't filled in up above.\n",
    "    if 'short_name' in short_name:\n",
    "        short_name = 'MOD10A2'\n",
    "        version = '6'\n",
    "        time_start = '2001-01-01T00:00:00Z'\n",
    "        time_end = '2019-03-07T22:09:38Z'\n",
    "        bounding_box = ''\n",
    "        polygon = '-109,37,-102,37,-102,41,-109,41,-109,37'\n",
    "        filename_filter = '*A2019*'  # '*2019010204*'\n",
    "        url_list = []\n",
    "\n",
    "    if not url_list:\n",
    "        url_list = cmr_search(short_name, version, time_start, time_end,\n",
    "                              bounding_box=bounding_box,\n",
    "                              polygon=polygon, filename_filter=filename_filter)\n",
    "#     print(set([len(u) for u in url_list]))\n",
    "    url_list = [u for u in url_list if len(u)==88]\n",
    "    cmr_download(url_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-29 11:55:46,392 INFO Welcome to the CDS\n",
      "2020-10-29 11:55:46,392 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-pressure-levels-monthly-means\n",
      "2020-10-29 11:55:46,795 INFO Request is queued\n",
      "2020-10-29 11:55:48,088 INFO Request is running\n",
      "2020-10-29 12:00:08,691 INFO Request is completed\n",
      "2020-10-29 12:00:08,691 INFO Downloading http://136.156.133.41/cache-compute-0013/cache/data5/adaptor.mars.internal-1603925747.1556737-29925-5-8891e466-d093-4822-bc5d-013cddab1817.nc to data/ERA5/ozone_mass_mixing_ratio.nc (3.9G)\n",
      "2020-10-29 12:12:04,001 INFO Download rate 5.5M/s                                                                                                                                                                                           \n"
     ]
    }
   ],
   "source": [
    "import cdsapi\n",
    "\n",
    "c = cdsapi.Client()\n",
    "def download(variable):\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-pressure-levels-monthly-means',\n",
    "    {\n",
    "        'format': 'netcdf',\n",
    "        'product_type': 'monthly_averaged_reanalysis',\n",
    "        'variable': 'ozone_mass_mixing_ratio',\n",
    "        'pressure_level': [\n",
    "            '50', '100',\n",
    "        ],\n",
    "        'year': [\n",
    "            '1979', '1980', '1981',\n",
    "            '1982', '1983', '1984',\n",
    "            '1985', '1986', '1987',\n",
    "            '1988', '1989', '1990',\n",
    "            '1991', '1992', '1993',\n",
    "            '1994', '1995', '1996',\n",
    "            '1997', '1998', '1999',\n",
    "            '2000', '2001', '2002',\n",
    "            '2003', '2004', '2005',\n",
    "            '2006', '2007', '2008',\n",
    "            '2009', '2010', '2011',\n",
    "            '2012', '2013', '2014',\n",
    "            '2015', '2016', '2017',\n",
    "            '2018', '2019', '2020',\n",
    "        ],\n",
    "        'month': [\n",
    "            '01', '02', '03',\n",
    "            '04', '05', '06',\n",
    "            '07', '08', '09',\n",
    "            '10', '11', '12',\n",
    "        ],\n",
    "        'time': '00:00',\n",
    "    },\n",
    "        f'data/ERA5/{variable}.nc')\n",
    "for variable in [\n",
    "                'ozone_mass_mixing_ratio', \n",
    "#                 'temperature', \n",
    "#                 'geopotential',\n",
    "#                 'u_component_of_wind',\n",
    "#                 'v_component_of_wind',\n",
    "]:\n",
    "    try:\n",
    "        download(variable)\n",
    "    except:\n",
    "        print(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "\n",
    "c = cdsapi.Client()\n",
    "def download(variable):\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-single-levels-monthly-means',\n",
    "        {\n",
    "            'format': 'netcdf',\n",
    "            'product_type': 'monthly_averaged_reanalysis',\n",
    "            'variable': [variable],\n",
    "            'year': [\n",
    "                '1979', '1980', '1981',\n",
    "                '1982', '1983', '1984',\n",
    "                '1985', '1986', '1987',\n",
    "                '1988', '1989', '1990',\n",
    "                '1991', '1992', '1993',\n",
    "                '1994', '1995', '1996',\n",
    "                '1997', '1998', '1999',\n",
    "                '2000', '2001', '2002',\n",
    "                '2003', '2004', '2005',\n",
    "                '2006', '2007', '2008',\n",
    "                '2009', '2010', '2011',\n",
    "                '2012', '2013', '2014',\n",
    "                '2015', '2016', '2017',\n",
    "                '2018', '2019', '2020',\n",
    "            ],\n",
    "            'month': [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "            ],\n",
    "            'time': '00:00',\n",
    "        },\n",
    "        f'data/ERA5/{variable}.nc')\n",
    "for variable in [\n",
    "                '10m_u_component_of_wind', '10m_v_component_of_wind', '10m_wind_speed',\n",
    "                '2m_temperature', 'sea_surface_temperature', 'skin_temperature',\n",
    "                'surface_net_solar_radiation', 'surface_pressure', 'surface_solar_radiation_downwards',\n",
    "            ]:\n",
    "    try:\n",
    "        download(variable)\n",
    "    except:\n",
    "        print(variable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
